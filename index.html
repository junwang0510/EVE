<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EVE</title>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="static/images/favicon.ico" type="image/x-icon">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<!-- Authors, Affiliations, Links -->
<body>
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            <img src="static/images/EVE_logo.png" alt="EVE logo" width="50">
            EVE: Enabling Anyone to Train Robots using Augmented Reality
          </h1>

          <div class="is-size-5 publication-authors">
            <div class="author-images">
              <span class="author-block">
                <img src="static/images/authors/jun.jpg" alt="Jun Wang" class="author-image">
              </span>
              <span class="author-block">
                <img src="static/images/authors/chang.jpg" alt="Chun-Cheng Chang" class="author-image">
              </span>
              <span class="author-block">
                <img src="static/images/authors/jiafei.jpg" alt="Jiafei Duan" class="author-image">
              </span>
              <span class="author-block">
                <img src="static/images/authors/dieter.jpg" alt="Dieter Fox" class="author-image">
              </span>
              <span class="author-block">
                <img src="static/images/authors/ranjay.jpg" alt="Ranjay Krishna" class="author-image">
              </span>
            </div>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://junwang0510.github.io/">Jun Wang<sup style="color: #555; font-size: 0.65em;">&spades;</sup></a>
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/chun-cheng-chang-28b49a223/">Chun-Cheng Chang<sup style="color: #555; font-size: 0.65em;">&spades;*</sup></a>
            </span>
            <span class="author-block">
              <a href="https://duanjiafei.com/">Jiafei Duan<sup style="color: #555; font-size: 0.65em;">&spades;*</sup></a>
            </span>
            <span class="author-block">
              <a href="https://homes.cs.washington.edu/~fox/">Dieter Fox<sup style="color: #555; font-size: 0.65em;">&spades;&diams;</sup></a>
            </span>
            <span class="author-block">
              <a href="https://www.ranjaykrishna.com/index.html">Ranjay Krishna<sup style="color: #555; font-size: 0.65em;">&spades;&clubs;</sup></a>
            </span>
          </div>

          <div class="is-size-5 publication-affiliations">
            <span class="affiliation-block"><sup>&spades;</sup>University of Washington</span>
            <span class="affiliation-block"><sup>&diams;</sup>NVIDIA</span>
            <span class="affiliation-block"><sup>&clubs;</sup>Allen Institute for AI</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Arxiv Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2404.06089"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=U6-xrNNHX2g"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/junwang0510"
                   class="external-link button is-normal is-rounded is-dark" style="pointer-events: none; cursor: default;">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (coming soon)</span>
                  </a>
              </span>
            </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Abstract -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The increasing affordability of robot hardware is accelerating the integration of robots into everyday activities. However, training a robot to automate a task requires expensive trajectory data where a trained human annotator moves a physical robot to train it. Consequently, only those with access to robots produce demonstrations to train robots.
          </p>
          <p>
            In this work, we remove this restriction with EVE, an iOS app that enables everyday users to train robots using intuitive augmented reality visualizations, without needing a physical robot. With EVE, users can collect demonstrations by specifying waypoints with their hands, visually inspecting the environment for obstacles, modifying existing waypoints, and verifying collected trajectories.
          </p>
          <p>
            In a user study ($N=14$, $D=30$) consisting of three common tabletop tasks, EVE outperformed three state-of-the-art interfaces in success rate and was comparable to kinesthetic teaching—physically moving a physical robot—in completion time, usability, motion intent communication, enjoyment, and preference ($mean_{p}=0.30$). EVE allows users to train robots for personalized tasks, such as sorting desk supplies, organizing ingredients, or setting up board games. We conclude by enumerating limitations and design considerations for future AR-based demonstration collection systems for robotics.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Formative Study -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Formative User Study</h2>
        <div class="content has-text-justified">
          <p>
            To understand the opportunities and challenges of AR-based trajectory collection, we conduct a formative study with $10$ participants with varying experience in robotics, using state-of-the-art collection interfaces (kinesthetic teaching, teleoperation with a VR controller, <a href="https://arxiv.org/abs/2306.13818">AR2-D2</a>) and new AR visualizations (AR Kinesthetic Teaching, Path History, Invisible Robot).
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Formative Study Interfaces -->
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-arkt">
          <div class="video-title">Kinesthetic Teaching</div>
          <video poster="" id="kt" autoplay controls muted loop playsinline>
            <source src="./static/videos/kt.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-arkt">
          <div class="video-title">Teleoperation</div>
          <video poster="" id="teleop" autoplay controls muted loop playsinline>
            <source src="./static/videos/teleop.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-arkt">
          <div class="video-title">AR2-D2</div>
          <video poster="" id="ar2d2" autoplay controls muted loop playsinline>
            <source src="./static/videos/ar2d2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-arkt">
          <div class="video-title">AR Kinesthetic Teaching</div>
          <video poster="" id="arkt" autoplay controls muted loop playsinline>
            <source src="./static/videos/arkt.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-path">
          <div class="video-title">Path History</div>
          <video poster="" id="path" autoplay controls muted loop playsinline>
            <source src="./static/videos/path_history.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-invbot">
          <div class="video-title">Invisible Robot</div>
          <video poster="" id="invbot" autoplay controls muted loop playsinline>
            <source src="./static/videos/invisible_robot.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
            Participants were given up to five minutes to try each interface by pushing, picking, and placing various cubes. We conducted a semi-structured interview about their experience, focusing on what features they liked and disliked about each interface. We also brainstormed additional features for the AR-based collection interface. Participants completed surveys and rankings for usability, motion intent communication, user enjoyment, and user preference.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/comments.png" alt="user comments">
      <h2 class="subtitle has-text-centered">
        User comments about the six demonstration collection methods used in the formative study.
      </h2>
      <img src="static/images/formative_results.png" alt="formative study results" width="70%">
      <h2 class="subtitle has-text-centered">
        The mean and standard deviation of usability (SUS: 1-100), motion intent communication (ranking: 1-4), user enjoyment (ranking: 1-4), and user preference (ranking: 1-4) are provided. Statistical significance is indicated by $\:\hat{}\:$ for $p \lt 0.05$.
      </h2>
    </div>
  </div>
</section>

<!-- Evaluation Study -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Evaluation User Study</h2>
        <div class="content has-text-justified">
          <p>
            Informed by formative study findings and our own experiences using EVE, we upgraded EVE's prototype with seven additional system changes to address the usability challenges.
          </p>
          <p>
            We conducted a user study to evaluate the effectiveness of EVE compared to baseline interfaces for three common tabletop tasks. These tasks were designed based on the survey from <a href="https://behavior.stanford.edu/behavior-1k">BEHAVIOR-1K</a> asking participants, "What do you want robots to do for you?" We included obstacles in all tasks to simulate real-world environments where tables are often cluttered.
          </p>
          <ul>
            <li>
              Toggle switch: Participants guided the robot to push a switch, turning on a light bulb. This task is more challenging than the button press task from the AR2- D2 paper, as it requires millimeter-level precision for the robot's end-effector to touch a specific point on the switch.
            </li>
            <li>
              Sort food: Participants controlled the robot to grab a melon and place it inside a yellow bowl, then pick up a steak and put it into a basket. This task simulated sorting different items into specific locations.
            </li>
            <li>
              Sweep table: Participants directed the robot to sweep beans to a location marked by tape with a sponge. This task simulated cleaning debris from a table.
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Evaluation Study Interfaces -->
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-reselection">
          <div class="video-title">Robot Instantiation Reselection</div>
          <video poster="" id="arkt" autoplay controls muted loop playsinline>
            <source src="./static/videos/reselect.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-replay">
          <div class="video-title">Trajectory Replay</div>
          <video poster="" id="arkt" autoplay controls muted loop playsinline>
            <source src="./static/videos/replay.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-cam">
          <div class="video-title">Dynamic Camera View</div>
          <video poster="" id="arkt" autoplay controls muted loop playsinline>
            <source src="./static/videos/cam.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-handpos">
          <div class="video-title">Hand Position Projection</div>
          <video poster="" id="arkt" autoplay controls muted loop playsinline>
            <source src="./static/videos/handpos.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-joint">
          <div class="video-title">Joint Constraints Signifier</div>
          <video poster="" id="arkt" autoplay controls muted loop playsinline>
            <source src="./static/videos/joint.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-traj">
          <div class="video-title">Realistic Trajectory</div>
          <video poster="" id="arkt" autoplay controls muted loop playsinline>
            <source src="./static/videos/traj.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-gripper">
          <div class="video-title">Accurate Gripper Control</div>
          <video poster="" id="arkt" autoplay controls muted loop playsinline>
            <source src="./static/videos/gripper.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Evaluation Study Tasks -->
<section class="section">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-half has-text-centered">
        <div class="select is-rounded">
          <select id="task-select" onchange="updateTaskMedia()">
            <option value="switch">Toggle Switch</option>
            <option value="sort">Sort Food</option>
            <option value="sweep">Sweep Table</option>
          </select>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-one-third has-text-centered">
        <img id="task-image" src="static/images/switch.jpg" alt="Toggle Switch" class="task-image">
      </div>
      <div class="column is-one-third has-text-centered">
        <video id="task-video1" controls class="task-video" autoplay controls muted loop playsinline>
          <source src="static/videos/switch.mp4" type="video/mp4">
        </video>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
            Success rates and remaining time for each task were recorded for all interfaces. Upon completing the task collection with all interfaces, participants filled out the SUS survey and a form ranking motion intent communication, user enjoyment, and overall preference for the interfaces. We aimed to measure $10$ collection attempts for each task.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Evaluation Study Results -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered">
        <div class="column is-half">
          <img src="static/images/success.png" alt="success rate">
          <h2 class="subtitle has-text-justified">
            The mean task success rate (%) for all interfaces in the evaluation user study, which included three tasks with $10$ trials each, indicates that EVE achieved the highest success rate across all tasks.
          </h2>
        </div>
        <div class="column is-half">
          <img src="static/images/time.png" alt="remaining time">
          <h2 class="subtitle has-text-justified">
            The mean and the standard deviation of the remaining time (sec) for completing one demonstration for each task. EVE performed comparably to kinesthetic teaching, with an average difference of $5.1$ seconds across all tasks.
          </h2>
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column is-half">
          <img src="static/images/eval_1.png" alt="eval_1">
        </div>
        <div class="column is-half">
          <img src="static/images/eval_2.png" alt="eval_2">
        </div>
      </div>
      <h2 class="subtitle has-text-justified">
        EVE performed comparably to kinesthetic teaching in usability, motion intent communication, user enjoyment, and user preference. The mean and standard deviation of all measurements are shown. Usability is rated on a scale from 0-100, with higher scores indicating better usability. Motion intent communication, user enjoyment, and user preference are ranked from 1-4, with lower ranks indicating better performance. Statistical significance is indicated by $*$ for $p \lt 0.05$ compared to EVE.
      </h2>
    </div>
  </div>
</section>

<!-- Policy Evaluation -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Policy Evaluation: AR2-D2 vs. EVE</h2>
        <div class="content has-text-justified">
          <p>
            To assess the effectiveness of the collected demonstrations, we trained robot policies for the toggle switch task using data gathered by AR2-D2 and EVE. Like AR2-D2, EVE collects RGB-D data and waypoint coordinates during the data collection process. After collecting the data, we used <a href="https://peract.github.io/">Perceiver-Actor (PerAct)</a> to train a transformer-based language-guided behavior cloning policy. PerAct takes a 3D voxel observation and a language goal $(v, l)$ as inputs and produces discretized outputs for the translation, rotation, and gripper state of the end-effector. These outputs, in conjunction with a motion planner, enable the execution of tasks specified by the language goal. We trained the policy using $6$ demonstrations over $30,000$ iterations for each interface.
          </p>
          <p>
            The trained policies were evaluated with $30$ task rollouts for the toggle switch task. The policy trained with EVE-collected data achieved a $30%$ success rate, compared to a $20%$ success rate with AR2-D2-collected data. We emphasize that the toggle switch task is more challenging than the button press task from the AR2-D2 paper, as it requires millimeter-level precision for the robot's end-effector to touch a specific point on the switch to turn on a light bulb. The higher success rate with EVE-collected data may be attributed to the improved depth perception during collection, allowing users to adjust waypoints for more accurate trajectories.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-three-quarters has-text-centered">
        <div class="content">
          <div class="columns is-multiline is-centered">
            <div class="column is-half">
              <div class="video-title">AR2-D2 RGB-D</div>
              <video controls class="task-video" autoplay controls muted loop playsinline>
                <source src="./static/videos/ar2d2_rgbd.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column is-half">
              <div class="video-title">AR2-D2 Evaluation</div>
              <video controls class="task-video" autoplay controls muted loop playsinline>
                <source src="./static/videos/ar2d2_eval.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column is-half">
              <div class="video-title">EVE RGB-D</div>
              <video controls class="task-video" autoplay controls muted loop playsinline>
                <source src="./static/videos/eve_rgbd.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column is-half">
              <div class="video-title">EVE Evaluation</div>
              <video controls class="task-video" autoplay controls muted loop playsinline>
                <source src="./static/videos/eve_eval.mp4" type="video/mp4">
              </video>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- bibtex -->
<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{wang2024eve,
  author    = {Wang, Jun and Chang, Chun-Cheng and Duan, Jiafei and Fox, Dieter and Krishna, Ranjay},
  title     = {EVE: Enabling Anyone to Train Robots using Augmented Reality},
  journal   = {arXiv preprint arXiv:2404.06089},
  year      = {2024}
}</code></pre>
  </div>
</section> -->

<!-- credits -->
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
        Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>, modified by <a href="https://junwang0510.github.io/">Jun Wang</a>.
      </p>
    </div>
  </div>
</footer>
</body>
</html>
